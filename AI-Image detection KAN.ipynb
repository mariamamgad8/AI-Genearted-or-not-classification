{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##**KAN for AI-generated Image calssification**"
      ],
      "metadata": {
        "id": "pTiP3F5M-bYW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##install important libraries"
      ],
      "metadata": {
        "id": "OC23KNdn-pCn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install torch torchvision\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZjK1wDDMIvc",
        "outputId": "fa1d1e62-d1d9-43f7-cccb-d37c338be305"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Set up Environment"
      ],
      "metadata": {
        "id": "s1eDwf8E_Ey7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.models import resnet18\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import lr_scheduler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Set seed for reproducibility\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n"
      ],
      "metadata": {
        "id": "XJVortAWMQiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models, transforms, datasets\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import Subset"
      ],
      "metadata": {
        "id": "KM7--5sBzbgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, f1_score"
      ],
      "metadata": {
        "id": "_Td4g8aU0nTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import lr_scheduler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n"
      ],
      "metadata": {
        "id": "qIGtL1BoOPqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Dataset Preprocessing"
      ],
      "metadata": {
        "id": "e1CMgHhW_LrV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import kagglehub\n",
        "from torchvision import transforms,datasets\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "# Download the latest version of the dataset\n",
        "path = kagglehub.dataset_download(\"birdy654/cifake-real-and-ai-generated-synthetic-images\")\n",
        "\n",
        "\n",
        "# Define dataset directories\n",
        "test_dir = '/root/.cache/kagglehub/datasets/birdy654/cifake-real-and-ai-generated-synthetic-images/versions/3/test'\n",
        "train_dir = '/root/.cache/kagglehub/datasets/birdy654/cifake-real-and-ai-generated-synthetic-images/versions/3/train'\n",
        "\n",
        "# Print dataset files for sanity check\n",
        "print(\"Files in 'test' directory:\", os.listdir(test_dir))\n",
        "print(\"Files in 'train' directory:\", os.listdir(train_dir))\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize for ResNet input\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize\n",
        "])\n",
        "\n",
        "# 1. Load the full train dataset\n",
        "full_trainset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
        "\n",
        "# Limit the train dataset to 1000 images\n",
        "num_train_samples = 1000\n",
        "train_indices = torch.randperm(len(full_trainset))[:num_train_samples]  # Randomly select 1000 indices\n",
        "subset_trainset = torch.utils.data.Subset(full_trainset, train_indices)\n",
        "\n",
        "# Split the 1000 images into train (800) and validation (200)\n",
        "train_size = int(0.8 * num_train_samples)  # 80% -> 800 images\n",
        "val_size = num_train_samples - train_size  # 20% -> 200 images\n",
        "trainset, valset = random_split(subset_trainset, [train_size, val_size])\n",
        "\n",
        "# 2. Load the full test dataset\n",
        "full_testset = datasets.ImageFolder(root=test_dir, transform=transform)\n",
        "\n",
        "# Limit the test dataset to 1000 images\n",
        "num_test_samples = 1000\n",
        "test_indices = torch.randperm(len(full_testset))[:num_test_samples]  # Randomly select 1000 indices\n",
        "testset = torch.utils.data.Subset(full_testset, test_indices)\n",
        "\n",
        "# 3. Create DataLoaders\n",
        "train_loader = DataLoader(trainset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(valset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(testset, batch_size=32, shuffle=False)\n",
        "\n",
        "\n",
        "# 4. Print dataset sizes for confirmation\n",
        "print(f\"Total images in training subset: {len(trainset)}\")\n",
        "print(f\"Total images in validation subset: {len(valset)}\")\n",
        "print(f\"Total images in test subset: {len(testset)}\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1y-8b7y4MT2g",
        "outputId": "22cb90d5-6c52-4d4f-e6ca-0403c6151c65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.5)\n",
            "Files in 'test' directory: ['FAKE', 'REAL']\n",
            "Files in 'train' directory: ['FAKE', 'REAL']\n",
            "Total images in training subset: 800\n",
            "Total images in validation subset: 200\n",
            "Total images in test subset: 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##KAN implementation"
      ],
      "metadata": {
        "id": "spdarvMU_hBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models\n",
        "\n",
        "# KAN Class\n",
        "class KAN(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_features,\n",
        "        out_features,\n",
        "        grid_size=5,\n",
        "        spline_order=3,\n",
        "        scale_noise=0.1,\n",
        "        scale_base=1.0,\n",
        "        scale_spline=1.0,\n",
        "        base_activation=torch.nn.SiLU,\n",
        "        grid_eps=0.02,\n",
        "        grid_range=[-1, 1],\n",
        "    ):\n",
        "        super(KAN, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.grid_size = grid_size\n",
        "        self.spline_order = spline_order\n",
        "\n",
        "        h = (grid_range[1] - grid_range[0]) / grid_size\n",
        "        grid = (\n",
        "            (\n",
        "                torch.arange(-spline_order, grid_size + spline_order + 1) * h\n",
        "                + grid_range[0]\n",
        "            )\n",
        "            .expand(in_features, -1)\n",
        "            .contiguous()\n",
        "        )\n",
        "        self.register_buffer(\"grid\", grid)\n",
        "\n",
        "        self.base_weight = torch.nn.Parameter(torch.Tensor(out_features, in_features))\n",
        "        self.spline_weight = torch.nn.Parameter(\n",
        "            torch.Tensor(out_features, in_features, grid_size + spline_order)\n",
        "        )\n",
        "\n",
        "        self.scale_noise = scale_noise\n",
        "        self.scale_base = scale_base\n",
        "        self.scale_spline = scale_spline\n",
        "        self.base_activation = base_activation()\n",
        "        self.grid_eps = grid_eps\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        torch.nn.init.constant_(self.base_weight, self.scale_base)\n",
        "        with torch.no_grad():\n",
        "            noise = (\n",
        "                (\n",
        "                    torch.rand(self.grid_size + 1, self.in_features, self.out_features)\n",
        "                    - 1 / 2\n",
        "                )\n",
        "                * self.scale_noise\n",
        "                / self.grid_size\n",
        "            )\n",
        "            self.spline_weight.data.copy_(\n",
        "                self.scale_spline\n",
        "                * self.curve2coeff(\n",
        "                    self.grid.T[self.spline_order : -self.spline_order],\n",
        "                    noise,\n",
        "                )\n",
        "            )\n",
        "\n",
        "    def b_splines(self, x: torch.Tensor):\n",
        "        assert x.dim() == 2 and x.size(1) == self.in_features\n",
        "\n",
        "        grid: torch.Tensor = self.grid\n",
        "        x = x.unsqueeze(-1)\n",
        "        bases = ((x >= grid[:, :-1]) & (x < grid[:, 1:])).to(x.dtype)\n",
        "        for k in range(1, self.spline_order + 1):\n",
        "            bases = (\n",
        "                (x - grid[:, : -(k + 1)])\n",
        "                / (grid[:, k:-1] - grid[:, : -(k + 1)])\n",
        "                * bases[:, :, :-1]\n",
        "            ) + (\n",
        "                (grid[:, k + 1 :] - x)\n",
        "                / (grid[:, k + 1 :] - grid[:, 1:(-k)])\n",
        "                * bases[:, :, 1:]\n",
        "            )\n",
        "\n",
        "        assert bases.size() == (\n",
        "            x.size(0),\n",
        "            self.in_features,\n",
        "            self.grid_size + self.spline_order,\n",
        "        )\n",
        "        return bases.contiguous()\n",
        "\n",
        "    def curve2coeff(self, x: torch.Tensor, y: torch.Tensor):\n",
        "        assert x.dim() == 2 and x.size(1) == self.in_features\n",
        "        assert y.size() == (x.size(0), self.in_features, self.out_features)\n",
        "\n",
        "        A = self.b_splines(x).transpose(0, 1)\n",
        "        B = y.transpose(0, 1)\n",
        "        solution = torch.linalg.lstsq(A, B).solution\n",
        "        result = solution.permute(2, 0, 1)\n",
        "\n",
        "        assert result.size() == (\n",
        "            self.out_features,\n",
        "            self.in_features,\n",
        "            self.grid_size + self.spline_order,\n",
        "        )\n",
        "        return result.contiguous()\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        assert x.dim() == 2 and x.size(1) == self.in_features\n",
        "\n",
        "        base_output = F.linear(self.base_activation(x), self.base_weight)\n",
        "        spline_output = F.linear(\n",
        "            self.b_splines(x).view(x.size(0), -1),\n",
        "            self.spline_weight.view(self.out_features, -1),\n",
        "        )\n",
        "        return base_output + spline_output\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GPor4naeMx0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model definition"
      ],
      "metadata": {
        "id": "jvqZEV4m_xPM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Definition with ResNet18 + KAN\n",
        "class KANResNet18(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(KANResNet18, self).__init__()\n",
        "        self.resnet = models.resnet18(pretrained=True)\n",
        "        in_features = self.resnet.fc.in_features\n",
        "        self.resnet.fc = KAN(in_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.resnet(x)\n"
      ],
      "metadata": {
        "id": "JkxfMPO3M1oR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Define Training Function"
      ],
      "metadata": {
        "id": "MSN3obkH_7Rp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Function\n",
        "def train(model, loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "    return running_loss / total, correct / total"
      ],
      "metadata": {
        "id": "YPLMAbBERpwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Define Validation Function"
      ],
      "metadata": {
        "id": "Zh0R2wlSADZ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation Function\n",
        "def validate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "    return running_loss / total, correct / total\n"
      ],
      "metadata": {
        "id": "wKhEbNx4zv7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training the Model"
      ],
      "metadata": {
        "id": "4A_W9dCzAKGG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Main Training Loop\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = KANResNet18(num_classes=2).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
        "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
        "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyVjvDL2zzDE",
        "outputId": "c40219d4-a173-4786-e697-1f60a9bd4568"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10] Train Loss: 0.4631, Train Acc: 0.7788, Val Loss: 0.7292, Val Acc: 0.6700\n",
            "Epoch [2/10] Train Loss: 0.3464, Train Acc: 0.8475, Val Loss: 0.5895, Val Acc: 0.7600\n",
            "Epoch [3/10] Train Loss: 0.2543, Train Acc: 0.8988, Val Loss: 0.9409, Val Acc: 0.7250\n",
            "Epoch [4/10] Train Loss: 0.3054, Train Acc: 0.8675, Val Loss: 0.7764, Val Acc: 0.7700\n",
            "Epoch [5/10] Train Loss: 0.1846, Train Acc: 0.9287, Val Loss: 1.2411, Val Acc: 0.6900\n",
            "Epoch [6/10] Train Loss: 0.2290, Train Acc: 0.9075, Val Loss: 0.3224, Val Acc: 0.8750\n",
            "Epoch [7/10] Train Loss: 0.1744, Train Acc: 0.9263, Val Loss: 0.3311, Val Acc: 0.8550\n",
            "Epoch [8/10] Train Loss: 0.1588, Train Acc: 0.9300, Val Loss: 0.5008, Val Acc: 0.8150\n",
            "Epoch [9/10] Train Loss: 0.1199, Train Acc: 0.9537, Val Loss: 0.4418, Val Acc: 0.8750\n",
            "Epoch [10/10] Train Loss: 0.1122, Train Acc: 0.9525, Val Loss: 0.3172, Val Acc: 0.9000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Testing the Model"
      ],
      "metadata": {
        "id": "o2HfYfZ9APC6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Loop\n",
        "test_loss, test_acc, test_precision, test_f1 = validate(model, test_loader, criterion, device)\n",
        "print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}, Test Precision: {test_precision:.4f}, Test F1: {test_f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnZUQtZzz_vm",
        "outputId": "9ded9ee1-64fe-4fcf-f71f-866b1293e4bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.3383, Test Acc: 0.8820, Test Precision: 0.8842, Test F1: 0.8819\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}